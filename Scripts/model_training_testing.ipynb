{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 13794948,
          "sourceType": "datasetVersion",
          "datasetId": 8782711
        }
      ],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_images(path):\n",
        "    exts = {\".png\", \".jpg\", \".jpeg\", \".webp\"}\n",
        "    count = 0\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for f in files:\n",
        "            if os.path.splitext(f.lower())[1] in exts:\n",
        "                count += 1\n",
        "    return count\n",
        "\n",
        "real_count = count_images(\"/kaggle/input/human-faces/real_images/content/dataset/real_images\")\n",
        "fake_count = count_images(\"/kaggle/input/human-faces/fake_images/content/dataset/fake_images\")\n",
        "\n",
        "print(\"Real images:\", real_count)\n",
        "print(\"Fake images:\", fake_count)\n",
        "print(\"Total:\", real_count + fake_count)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-19T22:03:06.504461Z",
          "iopub.execute_input": "2025-11-19T22:03:06.505247Z",
          "iopub.status.idle": "2025-11-19T22:03:53.925942Z",
          "shell.execute_reply.started": "2025-11-19T22:03:06.505218Z",
          "shell.execute_reply": "2025-11-19T22:03:53.925117Z"
        },
        "id": "rn9M5_aH_fXd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-19T22:03:53.927291Z",
          "iopub.execute_input": "2025-11-19T22:03:53.927494Z",
          "iopub.status.idle": "2025-11-19T22:04:01.614968Z",
          "shell.execute_reply.started": "2025-11-19T22:03:53.927477Z",
          "shell.execute_reply": "2025-11-19T22:04:01.614271Z"
        },
        "id": "wYg1wQPC_fXf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "DATA_DIR = '/kaggle/input/human-faces'\n",
        "REAL_DIR = os.path.join(DATA_DIR, 'real_images')\n",
        "FAKE_DIR = os.path.join(DATA_DIR, 'fake_images')\n",
        "\n",
        "# Image transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Custom dataset class\n",
        "class ImageFolderWithLabel(datasets.ImageFolder):\n",
        "    def __init__(self, root, transform=None):\n",
        "        super().__init__(root=root, transform=transform)\n",
        "        # Ensure 'Real' = 0, 'Fake' = 1\n",
        "        self.class_to_idx = {'real_images': 0, 'fake_images': 1}\n",
        "\n",
        "# Combine datasets\n",
        "all_data = ImageFolderWithLabel(DATA_DIR, transform=transform_train)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-19T22:04:01.615705Z",
          "iopub.execute_input": "2025-11-19T22:04:01.616017Z",
          "iopub.status.idle": "2025-11-19T22:04:10.836828Z",
          "shell.execute_reply.started": "2025-11-19T22:04:01.615990Z",
          "shell.execute_reply": "2025-11-19T22:04:10.836005Z"
        },
        "id": "teWHp1Uo_fXf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(all_data))\n",
        "test_size = len(all_data) - train_size\n",
        "train_dataset, test_dataset = random_split(all_data, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-19T22:04:10.837620Z",
          "iopub.execute_input": "2025-11-19T22:04:10.837828Z",
          "iopub.status.idle": "2025-11-19T22:04:10.844991Z",
          "shell.execute_reply.started": "2025-11-19T22:04:10.837809Z",
          "shell.execute_reply": "2025-11-19T22:04:10.844292Z"
        },
        "id": "Fd-7Hff7_fXf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load pretrained ResNet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Replace final layer for binary classification\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-19T22:04:10.846722Z",
          "iopub.execute_input": "2025-11-19T22:04:10.847503Z",
          "iopub.status.idle": "2025-11-19T22:04:11.759456Z",
          "shell.execute_reply.started": "2025-11-19T22:04:10.847482Z",
          "shell.execute_reply": "2025-11-19T22:04:11.758725Z"
        },
        "id": "_z-g8Y1X_fXf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "train_model(model, train_loader, criterion, optimizer)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-19T22:04:11.760401Z",
          "iopub.execute_input": "2025-11-19T22:04:11.760710Z",
          "iopub.status.idle": "2025-11-19T22:31:56.881197Z",
          "shell.execute_reply.started": "2025-11-19T22:04:11.760681Z",
          "shell.execute_reply": "2025-11-19T22:31:56.880159Z"
        },
        "id": "r73iN_zo_fXf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds)\n",
        "    rec = recall_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall:    {rec:.4f}\")\n",
        "    print(f\"F1-score:  {f1:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    labels = [\"Real\", \"Fake\"]\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-19T22:31:56.882565Z",
          "iopub.execute_input": "2025-11-19T22:31:56.882845Z",
          "iopub.status.idle": "2025-11-19T22:33:26.156766Z",
          "shell.execute_reply.started": "2025-11-19T22:31:56.882816Z",
          "shell.execute_reply": "2025-11-19T22:33:26.156006Z"
        },
        "id": "wYLv5tF2_fXg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract embeddings from penultimate layer\n",
        "model.eval()\n",
        "embeddings = []\n",
        "labels_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        feat = model.avgpool(model.layer4(model.layer3(model.layer2(model.layer1(model.relu(model.bn1(model.conv1(images))))))))\n",
        "        feat = torch.flatten(feat, 1)\n",
        "        embeddings.append(feat.cpu().numpy())\n",
        "        labels_list.append(labels.numpy())\n",
        "\n",
        "embeddings = np.vstack(embeddings)\n",
        "labels_list = np.hstack(labels_list)\n",
        "\n",
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "emb_2d = tsne.fit_transform(embeddings)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(emb_2d[:,0], emb_2d[:,1], c=labels_list, cmap='coolwarm', alpha=0.6)\n",
        "plt.title(\"t-SNE of Image Embeddings\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-19T22:33:26.157856Z",
          "iopub.execute_input": "2025-11-19T22:33:26.158554Z",
          "iopub.status.idle": "2025-11-19T22:35:16.028264Z",
          "shell.execute_reply.started": "2025-11-19T22:33:26.158524Z",
          "shell.execute_reply": "2025-11-19T22:35:16.027355Z"
        },
        "id": "3H7yBVZd_fXg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "\n",
        "# 1️⃣ Grad-CAM class\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        self.hook_handles = []\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations = output.detach()\n",
        "        def backward_hook(module, grad_in, grad_out):\n",
        "            self.gradients = grad_out[0].detach()\n",
        "        self.hook_handles.append(self.target_layer.register_forward_hook(forward_hook))\n",
        "        self.hook_handles.append(self.target_layer.register_backward_hook(backward_hook))\n",
        "\n",
        "    def generate(self, input_tensor, class_idx=None):\n",
        "        self.model.zero_grad()\n",
        "        output = self.model(input_tensor)\n",
        "        if class_idx is None:\n",
        "            class_idx = output.argmax(dim=1).item()\n",
        "        target = output[0, class_idx]\n",
        "        target.backward()\n",
        "        weights = self.gradients.mean(dim=[2,3], keepdim=True)\n",
        "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
        "        cam = F.relu(cam)\n",
        "        cam = F.interpolate(cam, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
        "        cam = cam.squeeze().cpu().numpy()\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
        "        return cam\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        for handle in self.hook_handles:\n",
        "            handle.remove()\n",
        "\n",
        "\n",
        "# 2️⃣ Unnormalization for plotting\n",
        "inv_transform = transforms.Normalize(\n",
        "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "    std=[1/0.229, 1/0.224, 1/0.225]\n",
        ")\n",
        "\n",
        "# 3️⃣ Grad-CAM visualization function\n",
        "def show_gradcam_with_pred(image_tensor, true_label, model, target_layer):\n",
        "    gradcam = GradCAM(model, target_layer)\n",
        "    image = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    # Model prediction\n",
        "    output = model(image)\n",
        "    pred_label = output.argmax(dim=1).item()\n",
        "\n",
        "    cam = gradcam.generate(image, class_idx=pred_label)  # generate heatmap for predicted class\n",
        "\n",
        "    # Original image for plotting\n",
        "    orig_img = inv_transform(image_tensor).permute(1,2,0).cpu().numpy()\n",
        "    orig_img = np.clip(orig_img, 0, 1)\n",
        "\n",
        "    # Grad-CAM overlay\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    overlay = (0.5*heatmap/255 + 0.5*orig_img).clip(0,1)\n",
        "\n",
        "    # Plot side by side\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(orig_img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Original\\nTrue: {'Real' if true_label==0 else 'Fake'}\\nPred: {'Real' if pred_label==0 else 'Fake'}\")\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(overlay)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Grad-CAM Overlay\\nPredicted: {'Real' if pred_label==0 else 'Fake'}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-19T22:35:16.029620Z",
          "iopub.execute_input": "2025-11-19T22:35:16.029945Z",
          "iopub.status.idle": "2025-11-19T22:35:16.487043Z",
          "shell.execute_reply.started": "2025-11-19T22:35:16.029917Z",
          "shell.execute_reply": "2025-11-19T22:35:16.486417Z"
        },
        "id": "6mOZ86G9_fXg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab a batch\n",
        "images, labels = next(iter(test_loader))\n",
        "\n",
        "# Separate real and fake images\n",
        "real_indices = [i for i, l in enumerate(labels) if l==0]\n",
        "fake_indices = [i for i, l in enumerate(labels) if l==1]\n",
        "\n",
        "real_samples = [images[i] for i in real_indices[:3]]\n",
        "real_labels = [labels[i].item() for i in real_indices[:3]]\n",
        "\n",
        "fake_samples = [images[i] for i in fake_indices[:3]]\n",
        "fake_labels = [labels[i].item() for i in fake_indices[:3]]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-19T22:35:16.487742Z",
          "iopub.execute_input": "2025-11-19T22:35:16.488031Z",
          "iopub.status.idle": "2025-11-19T22:35:21.411961Z",
          "shell.execute_reply.started": "2025-11-19T22:35:16.488005Z",
          "shell.execute_reply": "2025-11-19T22:35:21.411049Z"
        },
        "id": "T6DQh3tK_fXg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "target_layer = model.layer4[-1]  # last conv layer for ResNet\n",
        "model.eval()\n",
        "\n",
        "# Real images\n",
        "for img, lbl in zip(real_samples, real_labels):\n",
        "    show_gradcam_with_pred(img, lbl, model, target_layer)\n",
        "\n",
        "# Fake images\n",
        "for img, lbl in zip(fake_samples, fake_labels):\n",
        "    show_gradcam_with_pred(img, lbl, model, target_layer)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-19T22:35:21.413244Z",
          "iopub.execute_input": "2025-11-19T22:35:21.413492Z",
          "iopub.status.idle": "2025-11-19T22:35:23.561305Z",
          "shell.execute_reply.started": "2025-11-19T22:35:21.413468Z",
          "shell.execute_reply": "2025-11-19T22:35:23.560590Z"
        },
        "id": "B8QcZgOp_fXg"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}